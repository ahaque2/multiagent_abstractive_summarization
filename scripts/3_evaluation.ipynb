{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf9b7077-8a1a-4d41-b7e1-d07e0fb39da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "from bert_score import BERTScorer\n",
    "from rouge_score import rouge_scorer\n",
    "import ast\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f029f6f9-272f-44c2-a17c-8bf11abaa49e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_chatpgt_scores(prompt):\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4o\",\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "os.environ[\"LAS_API_TOKEN\"] = \"a4adfef6de531c1c258c9eda6f9be5c7a5701e365c32babe72273f63214811df\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"LAS_API_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff4c3fd3-9fa0-41a7-9b05-4b57bbb6850d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fn(a, b):\n",
    "    \n",
    "    product = a*b\n",
    "    if(product > 25):\n",
    "        product = 25\n",
    "    den = abs(10 - (a+b)) + 1\n",
    "    \n",
    "    metric = product/den\n",
    "    \n",
    "    return metric\n",
    "\n",
    "def normalize(s, smin, smax):\n",
    "    \n",
    "    return (s - smin) / (smax - smin)\n",
    "\n",
    "def compute_coverage_metric(scores1, scores2):\n",
    "\n",
    "    if(len(scores1) != len(scores2)):\n",
    "        return None\n",
    "    \n",
    "    coverage_score = []\n",
    "    for s1, s2 in zip(scores1, scores2):\n",
    "        scr = normalize(fn(s1, s2), 0.222, 26)    ## 0.22 in the min and 26 is the max possible for 2 documents on a scale of 1-10. We use these to conduct min max normalization\n",
    "        # print(s1, s2, scr)\n",
    "        coverage_score.append(scr)\n",
    "\n",
    "    return statistics.mean(coverage_score)\n",
    "\n",
    "\n",
    "def compute_attribution_metric(scores1, scores2):\n",
    "\n",
    "    if(len(scores1) != len(scores2)):\n",
    "        return None\n",
    "    \n",
    "    attr_score_diff = [x-y for x, y in zip(scores1, scores2)]\n",
    "    \n",
    "    return abs(statistics.mean(attr_score_diff))\n",
    "\n",
    "\n",
    "def Convert(score):\n",
    "    \n",
    "    score = score.strip('][')\n",
    "    score = [int(x) for x in list(score.split(\",\"))]\n",
    "    return score\n",
    "\n",
    "def coverage_score(docs, summary):   ## docs: list of documents, summary: one summary for all documents\n",
    "\n",
    "    eval_prompt = open('../prompts/eval_coverage.txt', 'r').read()\n",
    "    prompt1 = eval_prompt + \"\\n\" + document + \": \" + docs[0] + \"\\n\\n=======\\n\\nSummary sentences: \" +  \"\\n-\".join(sent_tokenize(summary))\n",
    "    prompt2 = eval_prompt + \"\\n\" + document + \": \" + docs[1] + \"\\n\\n=======\\n\\nSummary sentences: \" +  \"\\n-\".join(sent_tokenize(summary))\n",
    "\n",
    "    scores1 = get_chatpgt_scores(prompt1)\n",
    "    scores1 = Convert(scores1)\n",
    "\n",
    "    scores2 = get_chatpgt_scores(prompt2)\n",
    "    scores2 = Convert(scores2)\n",
    "    \n",
    "    return scores1, scores2\n",
    "\n",
    "def get_bert_scores(docs, summary):\n",
    "    # BERTScore calculation\n",
    "    scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "    f1_scores = []\n",
    "    for d in docs:\n",
    "        P, R, F1 = scorer.score([summary], [d])\n",
    "        F1 = float(F1)\n",
    "        f1_scores.append(F1)\n",
    "        \n",
    "    return statistics.mean(f1_scores)\n",
    "\n",
    "\n",
    "def get_rouge_scores(docs, summary):\n",
    "    # BERTScore calculation\n",
    "    f1_scores = []\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
    "    for d in docs:\n",
    "        scr = scorer.score(d, summary)\n",
    "        f1_scores.append(scr['rouge1'].fmeasure)\n",
    "        \n",
    "    return statistics.mean(f1_scores)\n",
    "\n",
    "\n",
    "def get_compression_ratio(docs, summary):\n",
    "    \n",
    "    return len(summary.split(' '))/len(\" \".join(docs).split(' '))\n",
    "\n",
    "\n",
    "def get_rating_metric(docs, summary, neg_topic, pos_topic, topic_df):\n",
    "\n",
    "    topic = list(set(neg_topic).intersection(set(pos_topic)))[0]\n",
    "    description = topic_df[topic_df.Topic == topic].Description.values[0]\n",
    "\n",
    "    prompt = open('../prompts/review_rating_prompts.txt').read()\n",
    "    prompt = prompt.replace('{topic}', topic).replace('{description}', description)\n",
    "    rating_doc1 = int(get_chatpgt_scores(prompt + \"\\n\" + docs[0]))\n",
    "    rating_doc2 = int(get_chatpgt_scores(prompt + \"\\n\" + docs[1]))\n",
    "    rating_sum = int(get_chatpgt_scores(prompt + \"\\n\" + summary))\n",
    "\n",
    "    # print(rating_doc1, rating_doc2, rating_sum)\n",
    "    \n",
    "    expected_rating = (rating_doc1 + rating_doc2)/2\n",
    "    diff = abs(rating_sum - expected_rating)\n",
    "    \n",
    "    return diff/10\n",
    "\n",
    "\n",
    "def get_cosine_sim_scores(ans_doc1, ans_doc2, ans_sum):\n",
    "    \n",
    "    cos1, cos2, diff = [], [], []\n",
    "    \n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    doc1_emb = model.encode(ans_doc1.split(\"\\n\"))\n",
    "    doc2_emb = model.encode(ans_doc2.split(\"\\n\"))\n",
    "    sum_emb = model.encode(ans_sum.split(\"\\n\"))\n",
    "    \n",
    "    # print(doc1_emb.shape)\n",
    "    \n",
    "    for d1, d2, s in zip(doc1_emb, doc2_emb, sum_emb):\n",
    "        \n",
    "        # print(d1.shape)\n",
    "        cosA = 1 - distance.cosine(s, d1)\n",
    "        cosB = 1 - distance.cosine(s, d2)\n",
    "        diff.append(cosA - cosB)\n",
    "        cos1.append(cosA)\n",
    "        cos2.append(cosB)\n",
    "        \n",
    "    return abs(statistics.mean(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6ef57ac-c12a-47ff-b153-3114bb0ca33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rev_data():\n",
    "    \n",
    "    df = pd.read_csv('../results/hotel_review_summaries.csv')\n",
    "    # df = pd.read_csv('results/hotel_review_summaries.csv')\n",
    "    topics = ['Service', 'Cleanliness', 'Value', 'Sleep Quality', 'Rooms', 'Business service (e.g., internet access)', 'Check in / front desk']\n",
    "    topic_df = pd.read_csv('../data/topic_definitions.csv')\n",
    "    \n",
    "    return df, topics, topic_df\n",
    "\n",
    "\n",
    "def get_news_data():\n",
    "\n",
    "    df_news = pd.read_csv('../data/conflicting_news_dataset.csv')\n",
    "    df_summaries = pd.read_csv('../results/news_summaries.csv')\n",
    "    df_news_qa = pd.read_csv('../data/news_questions.csv')\n",
    "    \n",
    "    return df_news, df_summaries\n",
    "\n",
    "\n",
    "def evaluate_rev_sum(df, topic_df, approach):\n",
    "    \n",
    "    attr_corel, attr_scores, coverage_scores, bert_scores, rouge_scores, sum_rating, compression_ratio = [], [], [], [], [], [], []\n",
    "    scoresA, scoresB = [], []\n",
    "\n",
    "    for i, row in df.iloc[0:2].iterrows():\n",
    "\n",
    "        docs = [df.iloc[i]['rev_neg'], df.iloc[i]['rev_pos']]\n",
    "        summary = df.iloc[i][approach]\n",
    "\n",
    "        scores1, scores2 = coverage_score(docs, summary)\n",
    "        corel = np.corrcoef(scores1, scores2)[0,1]\n",
    "        cvg_scores = compute_coverage_metric(scores1, scores2)\n",
    "        # attr_scr = compute_attribution_metric(scores1, scores2)\n",
    "        attr_scr = normalize(compute_attribution_metric(scores1,scores2), 0, 9)\n",
    "        b_scores = get_bert_scores(docs, summary)\n",
    "        r_scores = get_rouge_scores(docs, summary)\n",
    "\n",
    "        rev_topic_neg = ast.literal_eval(df.iloc[0].attr_neg)\n",
    "        rev_topic_pos = ast.literal_eval(df.iloc[0].attr_pos)\n",
    "        sum_rating_score = get_rating_metric(docs, summary, rev_topic_neg, rev_topic_pos, topic_df)\n",
    "\n",
    "        comp_ratio = get_compression_ratio(docs, summary)\n",
    "\n",
    "        attr_scores.append(attr_scr)\n",
    "        attr_corel.append(corel)\n",
    "        coverage_scores.append(cvg_scores)\n",
    "        bert_scores.append(b_scores)\n",
    "        rouge_scores.append(r_scores)\n",
    "        sum_rating.append(sum_rating_score)\n",
    "        compression_ratio.append(comp_ratio)\n",
    "\n",
    "        scoresA.append(scores1)\n",
    "        scoresB.append(scores2)\n",
    "        \n",
    "    return attr_corel, attr_scores, coverage_scores, bert_scores, rouge_scores, sum_rating, compression_ratio, scoresA, scoresB\n",
    "\n",
    "\n",
    "def evaluate_news_sum(approach, df_news, df_summaries):\n",
    "\n",
    "    attr_corel, attr_scores, coverage_scores, bert_scores, rouge_scores, qa_cos_diff, compression_ratio = [], [], [], [], [], [], []\n",
    "    scoresA, scoresB = [], []\n",
    "    \n",
    "    ## Iterating over each cluster. This is based on the evaluation dataset used (which has 15 clusters numbered from 1-15)\n",
    "    for c in range(1, 2):\n",
    "\n",
    "        news = df_news[df_news.cluster_id == c]\n",
    "        docs = [news[news.side == 'L'].text.values[0], news[news.side == 'R'].text.values[0]]\n",
    "\n",
    "        summary = df_summaries[df_summaries.cluster == c][approach + '_summary'].values[0]\n",
    "\n",
    "        Q = df_summaries[df_summaries.cluster == c]['questions']\n",
    "        # print(Q.values[0])\n",
    "        # sys.exit()\n",
    "        qa_prompt = open('../prompts/question_answering_prompt.txt').read()\n",
    "\n",
    "        qa_prompt_doc1 = qa_prompt + \"\\n\" + document + \":\\n\" + docs[0] + \"\\n\\n=======\\n\\nQuestions about the news:\\n\" + Q.values[0]\n",
    "        qa_prompt_doc2 = qa_prompt + \"\\n\" + document + \":\\n\" + docs[1] + \"\\n\\n=======\\n\\nQuestions about the news:\\n\" + Q.values[0]\n",
    "        qa_prompt_sum = qa_prompt + \"\\n\" + document + \":\\n\" + summary + \"\\n\\n=======\\n\\nQuestions about the news:\\n\" + Q.values[0]\n",
    "\n",
    "        ans_doc1 = get_chatpgt_scores(qa_prompt_doc1)\n",
    "        ans_doc2 = get_chatpgt_scores(qa_prompt_doc2)\n",
    "        ans_sum = get_chatpgt_scores(qa_prompt_sum)\n",
    "\n",
    "        cos_diff = get_cosine_sim_scores(ans_doc1, ans_doc2, ans_sum)\n",
    "        # print(\"cos_diff\", cos_diff)\n",
    "\n",
    "        scores1, scores2 = coverage_score(docs, summary)\n",
    "\n",
    "        corel = np.corrcoef(scores1, scores2)[0,1]\n",
    "        cvg_scores = compute_coverage_metric(scores1, scores2)\n",
    "        # attr_scr = compute_attribution_metric(scores1, scores2)\n",
    "        attr_scr = normalize(compute_attribution_metric(scores1,scores2), 0, 9)\n",
    "        b_scores = get_bert_scores(docs, summary)\n",
    "        r_scores = get_rouge_scores(docs, summary)\n",
    "\n",
    "        # rev_topic_neg = ast.literal_eval(df.iloc[0].attr_neg)\n",
    "        # rev_topic_pos = ast.literal_eval(df.iloc[0].attr_pos)\n",
    "        # sum_rating_score = get_rating_metric(docs, summary, rev_topic_neg, rev_topic_pos)\n",
    "\n",
    "        comp_ratio = get_compression_ratio(docs, summary)\n",
    "\n",
    "        attr_corel.append(corel)\n",
    "        attr_scores.append(attr_scr)\n",
    "        coverage_scores.append(cvg_scores)\n",
    "        bert_scores.append(b_scores)\n",
    "        rouge_scores.append(r_scores)\n",
    "        qa_cos_diff.append(cos_diff)\n",
    "        compression_ratio.append(comp_ratio)\n",
    "        \n",
    "        scoresA.append(scores1)\n",
    "        scoresB.append(scores2)\n",
    "        \n",
    "    return attr_corel, attr_scores, coverage_scores, bert_scores, rouge_scores, qa_cos_diff, compression_ratio, scoresA, scoresB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77965319-1c97-4740-b358-b4e1bb0dd5a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahaque2/venv/py3_scads_venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/ahaque2/venv/py3_scads_venv/lib/python3.10/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ahaque2/venv/py3_scads_venv/lib/python3.10/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "### Change the document variable to prduce evaluation metrics for news and reviews\n",
    "document = 'News'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "if(document == 'News'):\n",
    "    \n",
    "    ## Specify the approach here\n",
    "    ## Approaches can be [mutliagent, or writer_critic]\n",
    "    approach = 'writer_critic'\n",
    "    df, df_summaries = get_news_data()\n",
    "    attr_corel, attr_scores, coverage_scores, bert_scores, rouge_scores, sum_rating, compression_ratio, scoresA, scoresB = evaluate_news_sum(approach, df, df_summaries)\n",
    "    \n",
    "elif(document == 'Reviews'):\n",
    "    \n",
    "    ## Specify the approach here\n",
    "    ## Approaches can be [mutliagent, or writer_critic]\n",
    "    approach = 'mutliagent'\n",
    "    df, topics, topic_df = get_rev_data()\n",
    "    attr_corel, attr_scores, coverage_scores, bert_scores, rouge_scores, sum_rating, compression_ratio, scoresA, scoresB = evaluate_rev_sum(df, topic_df, approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0c137e0d-8cb9-46fa-9be7-c959310a401f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attr_corel, attr_scores, coverage_scores, bert_scores, rouge_scores, sum_rating, compression_ratio, scoresA, scoresB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5db41d81-c3d3-4ba4-afb2-c5e4fbf6bec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summaries = df_summaries[['cluster', 'multiagent_summary', 'writer_critic_summary', 'questions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "38175802-02b3-4322-a71a-c45d26570cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1958576/978253951.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_summaries['corelation_attr_' + approach] = attr_corel\n"
     ]
    }
   ],
   "source": [
    "approach = 'WC'   # Short hand notation for the approach to be used as column header for the metrics\n",
    "df_summaries['corelation_attr_' + approach] = attr_corel\n",
    "df_summaries['attribution_' + approach] = attr_scores\n",
    "df_summaries['coverage_' + approach] = coverage_scores \n",
    "df_summaries['bertScore_' + approach] = bert_scores\n",
    "df_summaries['rougeScore_' + approach] = rouge_scores\n",
    "df_summaries['qa_cos_diff_' + approach] = sum_rating\n",
    "df_summaries['compression_ratio_' + approach] = compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f7cd8d6-0adc-43ee-a873-f9617f8a8b72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'rev_neg', 'rev_pos', 'attr_neg', 'attr_pos', 'cls',\n",
       "       'multiagent_summary', 'writer_critic_summary', 'corelation_attr_WC',\n",
       "       'attribution_WC', 'coverage_WC', 'bertScore_WC', 'rougeScore_WC',\n",
       "       'rating_offset_WC', 'compression_ratio_WC', 'corelation_attr_MA',\n",
       "       'attribution_MA', 'coverage_MA', 'bertScore_MA', 'rougeScore_MA',\n",
       "       'rating_offset_MA', 'compression_ratio_MA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d1a0c93-e693-41f1-b5d4-c63184a478ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.round(3).to_csv('../results/summary_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27d0d635-8614-4596-a866-2c5d8eac17d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('results/hotel_review_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d356e5be-f0e6-4dc0-b978-9d5e9db70fef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WilcoxonResult(statistic=244.0, pvalue=4.743994003612866e-05)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "### Comparing the distribution of attribution scores. \n",
    "### Attribution scores are computed pairwise (on attribution score for each sentence for each reference document)\n",
    "### We compare the distribution toegteher for the entire evaluation dataset (due to less number of attribution scores per summary)\n",
    "res = wilcoxon(scoresA, scoresB, alternative='two-sided')\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_scads_venv",
   "language": "python",
   "name": "py3_scads_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
